# AI4Good Lab - Project Cere

*Montreal 2025*

## ğŸ§  Project Overview

This repository contains the work of **Project Cere** for the AI4Good Montreal program 2025 cohort. Our project focuses on developing and evaluating multimodal machine learning models that integrate visual, textual, and audio data to address real-world social challenges.


## Table of Contents

What to expect in this repository:

- [Installation and Usage](#installation-and-usage)
- [Repository Structure](#repository-structure)
- [Team Members](#team-members)
- [License](#license)
- [Acknowledgments](#acknowledgments)


## ğŸ“‚ Repository Structure


## ğŸš€ Installation & Usage

Using Python 3.10

Create virtual environment and install dependencies:

```bash
python -m venv brain-env
```
```bash
python -m pip install --upgrade pip
```

Activate the environement

```bash
# bash
source .brain-env/bin/activate
```
OR

```PowerShell
# PowerShell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
.\brain-env\Scripts\Activate.ps1
```


Then install the package:

```bash
# bash
pip install -e . # run each time multimodal_stimulus_fmri_predict is updated

```

Install the dependencies (optional):

```bash
#bash
pip install -r requirements.txt
```

### How to run the code


### Backup: Pulling Repo

```PowerShell
git status

git pull
```

## Project Roadmap

### Related Work

### Data

### Methodology

### Performance 

### Conclusion

## ğŸ‘¥ Team Members

[Team Member 1 Name] (GitHub Profile)

[Team Member 2 Name] (GitHub Profile)

[Team Member 3 Name] (GitHub Profile)

[Team Member 4 Name] (GitHub Profile)

## ğŸ“ License
This project is licensed under the [License Name] - see the LICENSE.md file for details.

## ğŸ™ Acknowledgments
AI4Good Montreal organizers and mentors

[Any other organizations or individuals you want to acknowledge]



-------------------------




# ğŸ§  Project Cere - Multimodal ML for Social Good

*AI4Good Lab Montreal â€¢ 2025 Cohort*

## ğŸŒŸ Project Overview

Project Cere develops **multimodal machine learning models** that integrate visual, textual, and audio data to address pressing social challenges. This repository contains our codebase, experiments, and documentation for creating interpretable AI systems with real-world impact.

## ğŸ“‹ Table of Contents

- [Installation](#-installation)
- [Repository Structure](#-repository-structure)
- [Usage](#-usage)
- [Project Roadmap](#-project-roadmap)
- [Team](#-team)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)


-------------------------


## ğŸ› ï¸ Installation

### Prerequisites

- Python 3.10+
- Git

### Setup
1. **Clone the repository**:

   ```bash
   git clone https://github.com/marialagakos/AI4Good-MTL-Group-2.git
   cd AI4Good-MTL-Group-2
   ```

2. **Create and activate virtual environment**:

   ```bash
   python -m venv cere-env
   # Linux/MacOS
   source cere-env/bin/activate
   # Windows (PowerShell)
   .\cere-env\Scripts\Activate.ps1
   ```

3. **Install dependencies**:

   ```bash
   pip install --upgrade pip
   pip install -e .  # Editable install for development
   pip install -r requirements.txt  # Optional: Full dependency install
   ```

## ğŸ“‚ Repository Structure

```
Project-Cere/
â”œâ”€â”€ data/                   # Raw and processed datasets
â”‚   â”œâ”€â”€ audio/              # Audio samples
â”‚   â”œâ”€â”€ text/               # Text corpora
â”‚   â””â”€â”€ visual/             # Image/video data
â”œâ”€â”€ models/                 # Pretrained models and checkpoints
â”œâ”€â”€ notebooks/              # Exploratory analysis and prototyping
â”œâ”€â”€ src/                    # Core source code
â”‚   â”œâ”€â”€ preprocessing/      # Data pipelines
â”‚   â”œâ”€â”€ modeling/           # Model architectures
â”‚   â””â”€â”€ evaluation/         # Metrics and analysis
â”œâ”€â”€ docs/                   # Technical documentation
â”œâ”€â”€ tests/                  # Unit and integration tests
â””â”€â”€ LICENSE.md
```

-------------------------


## ğŸš€ Usage

### Running the Pipeline

```bash
python src/main.py --modality all --config configs/default.yaml
```

### Key Arguments
- `--modality`: Choose `audio`, `text`, `visual`, or `all`
- `--config`: Path to YAML configuration file

### Jupyter Notebooks

```bash
jupyter lab notebooks/
```

## ğŸ—ºï¸ Project Roadmap

| Phase          | Key Deliverables                          |
|----------------|------------------------------------------|
| Data Analysis  | EDA reports, preprocessing pipelines     |
| Modeling       | Multimodal fusion architectures          |
| Evaluation     | Cross-modal attention visualizations     |
| Deployment     | Flask API for model serving              |

## ğŸ‘¥ Team

- [Jane Doe](https://github.com/janedoe) - Data Pipelines
- [John Smith](https://github.com/johnsmith) - Model Architecture
- [Alex Chen](https://github.com/alexchen) - Evaluation Metrics
- [Maria Garcia](https://github.com/mariagarcia) - Deployment

## ğŸ“œ License
This project is licensed under the **MIT License** - see [LICENSE.md](LICENSE.md) for details.

## ğŸ™ Acknowledgments

We gratefully acknowledge:

- The AI4Good Lab Montreal organizers
- Our project mentors and TAs.... 
- Compute Canada for providing advanced computing resources


