

# ğŸ§  Project Cere

#### Multimodal AI for Social Good

*AI4Good Lab @Mila â€¢ Montreal 2025 Cohort*

## ğŸ” Project Overview
 
Project Cere develops **multimodal machine learning models** that integrate visual, textual, and audio data to address pressing social challenges. This repository contains our codebase, experiments, and documentation for creating interpretable AI systems with real-world impact.

## âœ¨ Features

- **Modular Architecture**: Easy to extend with new classifiers
- **Multiple Classifier Types**: Classical ML, Neural Networks, and Ensemble methods
- **Flexible Data Pipeline**: Support for various fMRI data formats
- **Comprehensive Evaluation**: Cross-validation, metrics, and visualization
- **Hyperparameter Optimization**: Built-in grid search capabilities
- **Experiment Management**: YAML-based configuration system
- **Extensible Design**: Factory pattern for seamless classifier addition

## ğŸ“‹ Table of Contents

- [Installation](#-installation)
- [Repository Structure](#-repository-structure)
- [Features](#-features)
- [Usage](#-usage)
- [Project Roadmap](#-project-roadmap)
- [Team](#-team)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)


-------------------------


## ğŸ› ï¸ Installation

### Prerequisites

- Python 3.10+
- Git

### Setup
1. **Clone the repository**:

   ```bash
   git clone https://github.com/marialagakos/AI4Good-MTL-Group-2.git
   cd AI4Good-MTL-Group-2
   ```

2. **Create and activate virtual environment**:

   ```bash
   python -m venv cere-env
   # Linux/MacOS
   source cere-env/bin/activate
   # Windows (PowerShell)
   .\cere-env\Scripts\Activate.ps1
   ```

3. **Install dependencies**:

   ```bash
   pip install --upgrade pip
   pip install -e .  # Editable install for development
   pip install -r requirements.txt  # Optional: Full dependency install
   ```

## ğŸ“‚ Repository Structure

```
Project-Cere/
â”œâ”€â”€ main.py                 # Main execution script
â”œâ”€â”€ README.md               # Project documentation
â”œâ”€â”€ data/                   # Raw and processed datasets
â”‚   â”œâ”€â”€ feature_extraction.py    # Feature extraction utilities
â”‚   â”œâ”€â”€ DATA_INSTRUCTIONS.md
â”‚   â”œâ”€â”€ .ipnyb_checkpoints/
â”‚   â”œâ”€â”€ src/                 
â”‚   â”‚   â”œâ”€â”€ telepath/
â”‚   â”‚   â”œâ”€â”€ telepath.egg-info/
â”‚   â”‚   â””â”€â”€ temp_audio_chunks/
â”‚   â”œâ”€â”€ pca_data/
â”‚   â”œâ”€â”€ loaders.py            # loading (fmri, audio, text, visual)
â”‚   â”œâ”€â”€ preprocessors.py      # preprocessing
â”‚   â”œâ”€â”€ transforms.py         # transformations
â”‚   â”œâ”€â”€ fmri/                    # fMRI data
â”‚   â”œâ”€â”€ audio/                   # Audio samples
â”‚   â”œâ”€â”€ transcripts/             # Text corpora
â”‚   â””â”€â”€ visual/                  # Image/video data
â”œâ”€â”€ models/                    # Classifier implementations
â”‚   â”œâ”€â”€ base_classifier.py    # Abstract base class
â”‚   â”œâ”€â”€ classical/            # Traditional ML methods
â”‚   â”‚   â”œâ”€â”€ svm.py
â”‚   â”‚   â”œâ”€â”€ random_forest.py
â”‚   â”‚   â””â”€â”€ logistic_regression.py
â”‚   â”œâ”€â”€ neural/               # Neural network methods
â”‚   â”‚   â”œâ”€â”€ mlp.py
â”‚   â”‚   â”œâ”€â”€ cnn.py
â”‚   â”‚   â”œâ”€â”€ lstm.py
â”‚   â”‚   â””â”€â”€ transformer.py
â”‚   â””â”€â”€ ensemble/             # Ensemble methods
â”‚       â”œâ”€â”€ voting.py
â”‚       â””â”€â”€ stacking.py
â”œâ”€â”€ utils/                     # Utility functions
â”‚   â”œâ”€â”€ metrics.py            # Evaluation metrics
â”‚   â”œâ”€â”€ visualization.py     # Plotting functions
â”‚   â””â”€â”€ io_utils.py          # I/O operations
â”œâ”€â”€ experiments/              # Experiment management
â”‚   â”œâ”€â”€ experiment_runner.py
â”‚   â””â”€â”€ hyperparameter_search.py
â”œâ”€â”€ .gitignore              # File control
â”œâ”€â”€ docs/                   # Technical documentation
â”œâ”€â”€ tests/                  # Unit and integration tests
â””â”€â”€ LICENSE.md
```


-------------------------


## ğŸš€ Usage

### Running the Pipeline

```bash
python src/main.py --modality all --config configs/default.yaml
```

### Key Arguments

- `--modality`: Choose `audio`, `text`, `visual`, or `all`
- `--config`: Path to YAML configuration file

### Jupyter Notebooks

```bash
jupyter lab notebooks/
```

## ğŸ—ºï¸ Project Roadmap

| Phase          | Key Deliverables                          |
|----------------|------------------------------------------|
| Data Analysis  | EDA reports, preprocessing pipelines     |
| Modeling       | Multimodal fusion architectures          |
| Evaluation     | Cross-modal attention visualizations     |
| Deployment     | Flask API for model serving              |

## ğŸ‘¥ Team

- [Maria Lagakos](https://github.com/marialagakos) - Feature Extraction
- [Sophie Strassmann](https://github.com/sophiestrazie) - Creative Director, Pipeline Architecture, and Classification Team
- [Yujie Chen](https://github.com/huricaneee) - Classification Team
- [Keyu Liang](https://github.com/Keyu17) - Feature Extraction and Data Migration
- [Maria Gallamoso](https://github.com/mariagarcia) - Feature Extraction Team
- [Catherina Medeiros](https://github.com/cathmedeiros) Director of Imaging, Feature Extraction Team


## ğŸ“œ License

This project is licensed under the **MIT License** - see [LICENSE.md](LICENSE.md) for details.

## ğŸ™ Acknowledgments

We gratefully acknowledge:

- **Jennifer Addison** and **Yosra Kazemi** for their expertise and leadership
- The AI4Good Lab Montreal and Mila team for their support
- Our TA **Hugo Berard** and **Laetitia Constantin**

Consulting Scholars and Mentors:

- Rose Landry - Mila
- Adel Halawa - McGill University
- Dr. Lune Bellec - UniversitÃ© de MontrÃ©al
- Dr. Mayada Elsabbagh - Transforming Autism Care Consortium
- The Algonauts Project
- Compute Canada for their computational resources
- The Digital Research Alliance of Canada
